{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification\n",
    "In this exercise, we will be using logistic regression and neural networks to recognize handwritten digits (from 0 to 9). First, lets load in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(401,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadmat('ex3data1.mat')\n",
    "X = data['X']\n",
    "X = np.insert(X, 0, 1, axis=1) # insert column of 1s into X to account for bias\n",
    "y = data['y']\n",
    "y[y == 10] = 0 # replace all of the 10s with 0s\n",
    "theta = np.zeros(np.size(X, 1))\n",
    "num_labels = 10 # the number of classes\n",
    "display(X.shape, y.shape, theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at 100 rows from X and map each row to a 20 by 20 pixel grayscale image, we can display some of the images of handwritten numbers that we are working with. The diagram below is taken from the exercise's instruction pdf.\n",
    "\n",
    "<img src=\"data_image.png\" width=250></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin anything, we are first going to implement a completely vectorized version of logistic regression that does not use any `for` loops. This includes vectorizing the cost function and gradient function, and doing this for regularized logistic regression as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y):\n",
    "    m,n = X.shape\n",
    "    theta = np.reshape(theta, (n,1))\n",
    "    h = sigmoid(X @ theta)\n",
    "    h[h == 1] = 0.999 # log(1)=0 causes error in division; credit to Utku Ufuk for this line\n",
    "    inner = -y*np.log(h)-(1-y)*np.log(1-h)\n",
    "    return inner.sum()/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y): \n",
    "    m,n = X.shape\n",
    "    theta = np.reshape(theta, (n,1))\n",
    "    h = sigmoid(X @ theta)\n",
    "    inner = X.T @ (h - y) \n",
    "    return (inner/m).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(theta, X, y, regParam):\n",
    "    m,n = X.shape\n",
    "    theta = np.reshape(theta, (n,1))\n",
    "    h = sigmoid(X @ theta)\n",
    "    h[h == 1] = 0.999 # log(1)=0 causes error in division; credit to Utku Ufuk for this line\n",
    "    inner = -y*np.log(h)-(1-y)*np.log(1-h)\n",
    "    outer = inner.sum()/m\n",
    "    reg = regParam/(2*m) * (theta**2).sum()\n",
    "    return outer + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, regParam):\n",
    "    m,n = X.shape\n",
    "    theta = np.reshape(theta, (n,1))\n",
    "    h = sigmoid(X @ theta)\n",
    "    inner = X.T @ (h - y)\n",
    "    outer = inner/m\n",
    "    reg_theta = theta.copy() \n",
    "    reg_theta[0] = 0\n",
    "    reg = (regParam/m)*reg_theta\n",
    "    return (outer + reg).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cost and gradient functions ready, we can start implementing one-vs-all classification. How this works is that we are going to pass in X, y (which is a vector of numbers from 0 to 9), num_labels (the number of classes), and regParam (the regularization parameter). This function will loop through each possible value of an element in y (a number that was drawn) and modify y so that it is a vector of 1s and 0s, a 1 meaning that the element in the original y vector is the same as the current number the loop is focusing on. For example, if the loop is on the value 7, all the 7s in y will be 1 and all the non 7s will be 0. We will then pass this modified y vector into our optimization formula and return an array of n parameters. This array of parameters is optimized to return the probability that the input would be the value of the loop. \n",
    "\n",
    "This function will return a matrix of dimension $K \\times (N+1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a matrix where each row corresponds to the learned parameters for each class\n",
    "# K x (n+1)\n",
    "def oneVsAll(X, y, num_labels, regParam):\n",
    "    m,n = X.shape\n",
    "    classifiers = np.zeros((num_labels, n))\n",
    "    \n",
    "    for k in range(num_labels):\n",
    "        y_class = (y == k).astype(int)\n",
    "        theta = np.zeros(n) \n",
    "        classifiers[k] = optimize.fmin_cg(costReg, theta, fprime=gradientReg, args=(X, y_class, regParam))\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.022566\n",
      "         Iterations: 25\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 140\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.027534\n",
      "         Iterations: 35\n",
      "         Function evaluations: 173\n",
      "         Gradient evaluations: 162\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.069455\n",
      "         Iterations: 111\n",
      "         Function evaluations: 384\n",
      "         Gradient evaluations: 372\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.076384\n",
      "         Iterations: 29\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 126\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.052456\n",
      "         Iterations: 96\n",
      "         Function evaluations: 301\n",
      "         Gradient evaluations: 301\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.076862\n",
      "         Iterations: 140\n",
      "         Function evaluations: 377\n",
      "         Gradient evaluations: 377\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.035782\n",
      "         Iterations: 48\n",
      "         Function evaluations: 271\n",
      "         Gradient evaluations: 259\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.047091\n",
      "         Iterations: 65\n",
      "         Function evaluations: 259\n",
      "         Gradient evaluations: 248\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.102272\n",
      "         Iterations: 29\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 110\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.091598\n",
      "         Iterations: 70\n",
      "         Function evaluations: 276\n",
      "         Gradient evaluations: 264\n"
     ]
    }
   ],
   "source": [
    "optimal_classifiers = oneVsAll(X, y, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 401)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_classifiers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, finally we managed to have a set of optimal parameters for each class. Now lets see how accurate our parameters actually are. In order to do so, we first need to predict, for each input of 20x20 pixels, what number our multi class logistic regression would classify the pixels as. After gathering the predicted numbers, we can run them through a function to get the testing accuracy of our multi class logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, classifiers): \n",
    "    probabilities = sigmoid(X @ classifiers.T) # each column is the probability for that class \n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    return np.reshape(predictions, (np.size(predictions, 0),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingAccuracy(actual, predicted):\n",
    "    m = np.size(actual, 0)\n",
    "    numCorrect = 0\n",
    "    for i in range(m):\n",
    "        if actual[i][0] == predicted[i][0]:\n",
    "            numCorrect += 1\n",
    "    return numCorrect/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.941"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingAccuracy(y, predict(X, optimal_classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given an regularization parameter of 1, our multi class logistic regression model correctly classifies 94.1% of the numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, we will be implementing a neural network to recognize handwritten digits using the same training set as before. Our neural network will have 3 layers: the input layer, a hidden layer, and the output layer. The input layer will have 400 units (and then another one which is the bias unit). We have been provided with a set of network parameters $(\\Theta^{(1)}, \\Theta^{(2)})$ which have already been trained. These parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes).\n",
    "\n",
    "Let's load in these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_weights = loadmat('ex3weights.mat')\n",
    "theta1 = nn_weights['Theta1']\n",
    "theta2 = nn_weights['Theta2']\n",
    "display(X.shape, theta1.shape, theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement this neural network, we will be implementing the feedforward propagation method. Below is the implemented function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(input, theta1, theta2):\n",
    "    firstLayer = input.T # 401x5000\n",
    "    secondLayer = sigmoid(theta1 @ firstLayer) # 25x401 * 401x5000 = 25x5000 \n",
    "    secondLayer = np.insert(secondLayer, 0, 1, axis=0) # 26x5000\n",
    "    output = sigmoid(theta2 @ secondLayer) # 10x26 * 26x5000 = 10x5000\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12661530e-04, 4.79026796e-04, 8.85702310e-05, ...,\n",
       "        5.17641791e-02, 8.30631310e-04, 4.81465717e-05],\n",
       "       [1.74127856e-03, 2.41495958e-03, 3.24266731e-03, ...,\n",
       "        3.81715020e-03, 6.22003774e-04, 4.58821829e-04],\n",
       "       [2.52696959e-03, 3.44755685e-03, 2.55419797e-02, ...,\n",
       "        2.96297510e-02, 3.14518512e-04, 2.15146201e-05],\n",
       "       ...,\n",
       "       [4.01468105e-04, 2.39107046e-03, 6.22892325e-02, ...,\n",
       "        2.15667361e-03, 1.19366192e-02, 5.73434571e-03],\n",
       "       [6.48072305e-03, 1.97025086e-03, 5.49803551e-03, ...,\n",
       "        6.49826950e-01, 9.71410499e-01, 6.96288990e-01],\n",
       "       [9.95734012e-01, 9.95696931e-01, 9.28008397e-01, ...,\n",
       "        2.42384687e-05, 2.06173648e-04, 8.18576980e-02]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = feedForward(X, theta1, theta2)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this is similar to that of our multi class logistic regression. At each input value, it gives and array of probabilities of the input being that index number. We can rewrite a prediction function that predicts what each handwritten image is and then test the accuracy of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(h):\n",
    "    probabilities = h.T\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    predictions = (predictions + 1) % 10 # need this to shift over the results since the weights are assuming that 0 is represented as 10)\n",
    "    return np.reshape(predictions, (np.size(predictions, 0),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingAccuracy(y, predict2(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy of our neural network is around 97.5%!!!\n",
    "\n",
    "This concludes exercise 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
