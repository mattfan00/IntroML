{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "In the first part of this exercise we will be implementing feedforward propagation as we did in the second part of the previous exercise. Let's first load in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [9],\n",
       "       [9],\n",
       "       [9]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "X = np.insert(X, 0, 1, axis=1) # insert column of 1s into X to account for bias\n",
    "y = data['y']\n",
    "y[y == 10] = 0 # replace all of the 10s with 0s\n",
    "display(X.shape, y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous exercise, we are provided the initial weights. There are two sets of weights, meaning that there are a total of three layers in our neural network. Let's load these weights in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "theta1 = weights['Theta1']\n",
    "theta2 = weights['Theta2']\n",
    "display(theta1.shape, theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the cost function and gradient for the neural network. The function `feedForward` is used as a helper function to get the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(theta1, theta2, X):\n",
    "    input = X.T\n",
    "    second_layer = sigmoid(theta1 @ input)\n",
    "    second_layer = np.insert(second_layer, 0, 1, axis=0)\n",
    "    output = sigmoid(theta2 @ second_layer)\n",
    "    return output.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam):\n",
    "    m = np.size(X, 0)\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "    \n",
    "    h = np.roll(feedForward(theta1, theta2, X), 1, axis=1)\n",
    "    \n",
    "    J = 0\n",
    "    for k in range(num_labels):\n",
    "        y_class = ((y == k).astype(int)).T\n",
    "        J += (y_class*np.log(h[:,k])+(1-y_class)*np.log(1-h[:,k])).sum()\n",
    "    \n",
    "    theta1 = np.delete(theta1, 0, 1) # remove bias term\n",
    "    theta2 = np.delete(theta2, 0, 1) # remove bias term\n",
    "    reg = (regParam/(2*m))*((theta1**2).sum()+(theta2**2).sum())\n",
    "\n",
    "    return -J/m + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test out our cost function, we need to do a little bit of initialization first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.concatenate([theta1.flatten(), theta2.flatten()])\n",
    "input_layer_size = 400 # 20x20 matrix of pixels \n",
    "hidden_layer_size = 25 # 25 hidden layer units \n",
    "num_labels = 10 # 10 output units\n",
    "regParam = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2876291651613189"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost function correctly gives a correct cost of about 0.287629.\n",
    "\n",
    "Time to move onto implementing the steps necessary for backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "In order to implement the backpropagation algorithm, we have to implement the gradient for the neural network. Once we have computed the gradient, we will be able to train the neural network by minimizing the cost function $J(\\Theta)$ using an advanced optimizer.\n",
    "\n",
    "To get started, we will first have to implement the sigmoid gradient function. This function will be used when we are computing the \"error\" at each layer.\n",
    "\n",
    "The gradient for the sigmoid function is as follows: \n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGrad(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last step we need to do before we start implementing the backpropagation algorithm is to do some intialization. In particular we need to randomely initialize the weights for $\\Theta$, which will be in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. Random initialization is important for symmetry breaking, if all of the initial values are the same the algorithm won't improve on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_init = 0.12\n",
    "\n",
    "# L_in is the number of incoming connections\n",
    "# L_out is the number of outgoing connections\n",
    "def randInitWeights(L_in, L_out): \n",
    "    W = np.random.rand(L_out, L_in+1)\n",
    "    W = W * 2 * epsilon_init\n",
    "    W = W - epsilon_init\n",
    "    return W\n",
    "\n",
    "theta1 = randInitWeights(input_layer_size, hidden_layer_size)\n",
    "theta2 = randInitWeights(hidden_layer_size, num_labels)\n",
    "nn_params = np.concatenate([theta1.flatten(), theta2.flatten()]) # unroll the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, our time has finally come, time to implement the backpropagation algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam):\n",
    "    m = np.size(X, 0)\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "    D1 = np.zeros((hidden_layer_size, input_layer_size+1))\n",
    "    D2 = np.zeros((num_labels, hidden_layer_size+1))\n",
    "    \n",
    "    h = feedForward(theta1, theta2, X)\n",
    "    \n",
    "    J = 0\n",
    "    for k in range(num_labels):\n",
    "        y_class = ((y == k).astype(int)).T\n",
    "        J += (y_class*np.log(h[:,k])+(1-y_class)*np.log(1-h[:,k])).sum()\n",
    "    \n",
    "    theta1_temp = np.delete(theta1, 0, 1) # remove bias term\n",
    "    theta2_temp = np.delete(theta2, 0, 1) # remove bias term\n",
    "    reg = (regParam/(2*m))*((theta1_temp**2).sum()+(theta2_temp**2).sum())\n",
    "\n",
    "    J = -J/m + reg\n",
    "    \n",
    "    # loop through every training example\n",
    "    for t in range(m): \n",
    "        a1 = X[[t],:].T\n",
    "        z2 = theta1 @ a1\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.insert(a2, 0, 1, axis=0)\n",
    "        z3 = theta2 @ a2\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        yt = np.zeros((num_labels,1))\n",
    "        for k in range(num_labels):\n",
    "            if(k == y[t][0]):\n",
    "                yt[k] = 1\n",
    "            else:\n",
    "                yt[k] = 0\n",
    "                \n",
    "        d3 = a3 - yt\n",
    "        \n",
    "        z2 = np.insert(z2, 0, 1, axis=0)\n",
    "        d2 = (theta2.T @ d3)*sigmoidGrad(z2)\n",
    "        \n",
    "        D1 = D1 + (d2[1:] @ a1.T)\n",
    "        D2 = D2 + (d3 @ a2.T)\n",
    "        \n",
    "    D1 = D1/m\n",
    "    D2 = D2/m\n",
    "    \n",
    "    D1[:,1:] = D1[:,1:] + (theta1[:,1:]*regParam)/m\n",
    "    D2[:,1:] = D2[:,1:] + (theta2[:,1:]*regParam)/m\n",
    "    \n",
    "    grad = np.concatenate([D1.flatten(), D2.flatten()])\n",
    "    \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.228100074051737"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "J,grad = backProp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)\n",
    "display(J, grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "fmin = minimize(fun=backProp, x0=nn_params, args=(input_layer_size, hidden_layer_size, num_labels, X, y, regParam), \n",
    "                method='TNC', jac=True, options={'maxiter': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_weights = fmin.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now `opt_weights` should have the optimal weights so that we can correctly predict handwritten digits. Now is the moment of truth, lets test the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_theta1 = np.reshape(opt_weights[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "opt_theta2 = np.reshape(opt_weights[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "display(optimal_theta1.shape, optimal_theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = feedForward(opt_theta1, opt_theta2, X)\n",
    "\n",
    "def predict(h):\n",
    "    probabilities = h\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    return np.reshape(predictions, (np.size(predictions, 0),1))\n",
    "\n",
    "def testingAccuracy(actual, predicted):\n",
    "    m = np.size(actual, 0)\n",
    "    numCorrect = 0\n",
    "    for i in range(m):\n",
    "        if actual[i][0] == predicted[i][0]:\n",
    "            numCorrect += 1\n",
    "    return numCorrect/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingAccuracy(y, predict(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE MANAGED TO GET 99.66% ACCURACY\n",
    "\n",
    "The computations really made the fans on my laptop spin like crazy, but as you can see the neural network works!\n",
    "\n",
    "Credit to John Wittenauer ([link](https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/)) for inspiring some of my code, especially the minimization function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
