{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "In the first part of this exercise we will be implementing feedforward propagation as we did in the second part of the previous exercise. Let's first load in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "X = np.insert(X, 0, 1, axis=1) # insert column of 1s into X to account for bias\n",
    "y = data['y']\n",
    "y[y == 10] = 0 # replace all of the 10s with 0s\n",
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous exercise, we are provided the initial weights. There are two sets of weights, meaning that there are a total of three layers in our neural network. Let's load these weights in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "theta1 = weights['Theta1']\n",
    "theta2 = weights['Theta2']\n",
    "display(theta1.shape, theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the cost function and gradient for the neural network. The function `feedForward` is used as a helper function to get the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(theta1, theta2, X):\n",
    "    input = X.T\n",
    "    second_layer = sigmoid(theta1 @ input)\n",
    "    second_layer = np.insert(second_layer, 0, 1, axis=0)\n",
    "    output = sigmoid(theta2 @ second_layer)\n",
    "    return np.roll(output.T, 1, axis=1) # use np.roll since 0 is treated as 10 in MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam):\n",
    "    m = np.size(X, 0)\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "    \n",
    "    h = feedForward(theta1, theta2, X)\n",
    "    \n",
    "    J = 0\n",
    "    for k in range(num_labels):\n",
    "        y_class = ((y == k).astype(int)).T\n",
    "        J += (y_class*np.log(h[:,k])+(1-y_class)*np.log(1-h[:,k])).sum()\n",
    "    \n",
    "    theta1 = np.delete(theta1, 0, 1) # remove bias term\n",
    "    theta2 = np.delete(theta2, 0, 1) # remove bias term\n",
    "    reg = (regParam/(2*m))*((theta1**2).sum()+(theta2**2).sum())\n",
    "\n",
    "    return -J/m + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test out our cost function, we need to do a little bit of initialization first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.concatenate([theta1.flatten(), theta2.flatten()])\n",
    "input_layer_size = 400 # 20x20 matrix of pixels \n",
    "hidden_layer_size = 25 # 25 hidden layer units \n",
    "num_labels = 10 # 10 output units\n",
    "regParam = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2876291651613189"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost function correctly gives a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
