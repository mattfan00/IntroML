{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "In the first part of this exercise we will be implementing feedforward propagation as we did in the second part of the previous exercise. Let's first load in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [9],\n",
       "       [9],\n",
       "       [9]], dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "X = np.insert(X, 0, 1, axis=1) # insert column of 1s into X to account for bias\n",
    "y = data['y']\n",
    "y[y == 10] = 0 # replace all of the 10s with 0s\n",
    "display(X.shape, y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous exercise, we are provided the initial weights. There are two sets of weights, meaning that there are a total of three layers in our neural network. Let's load these weights in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "theta1 = weights['Theta1']\n",
    "theta2 = weights['Theta2']\n",
    "display(theta1.shape, theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement the cost function and gradient for the neural network. The function `feedForward` is used as a helper function to get the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(theta1, theta2, X):\n",
    "    input = X.T\n",
    "    second_layer = sigmoid(theta1 @ input)\n",
    "    second_layer = np.insert(second_layer, 0, 1, axis=0)\n",
    "    output = sigmoid(theta2 @ second_layer)\n",
    "    return np.roll(output.T, 1, axis=1) # use np.roll since 0 is treated as 10 in MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam):\n",
    "    m = np.size(X, 0)\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "    \n",
    "    h = feedForward(theta1, theta2, X)\n",
    "    \n",
    "    J = 0\n",
    "    for k in range(num_labels):\n",
    "        y_class = ((y == k).astype(int)).T\n",
    "        J += (y_class*np.log(h[:,k])+(1-y_class)*np.log(1-h[:,k])).sum()\n",
    "    \n",
    "    theta1 = np.delete(theta1, 0, 1) # remove bias term\n",
    "    theta2 = np.delete(theta2, 0, 1) # remove bias term\n",
    "    reg = (regParam/(2*m))*((theta1**2).sum()+(theta2**2).sum())\n",
    "\n",
    "    return -J/m + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test out our cost function, we need to do a little bit of initialization first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.concatenate([theta1.flatten(), theta2.flatten()])\n",
    "input_layer_size = 400 # 20x20 matrix of pixels \n",
    "hidden_layer_size = 25 # 25 hidden layer units \n",
    "num_labels = 10 # 10 output units\n",
    "regParam = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2876291651613189"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost function correctly gives a correct cost of about 0.287629.\n",
    "\n",
    "Time to move onto implementing the steps necessary for backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "In order to implement the backpropagation algorithm, we have to implement the gradient for the neural network. Once we have computed the gradient, we will be able to train the neural network by minimizing the cost function $J(\\Theta)$ using an advanced optimizer.\n",
    "\n",
    "To get started, we will first have to implement the sigmoid gradient function. This function will be used when we are computing the \"error\" at each layer.\n",
    "\n",
    "The gradient for the sigmoid function is as follows: \n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGrad(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last step we need to do before we start implementing the backpropagation algorithm is to do some intialization. In particular we need to randomely initialize the weights for $\\Theta$, which will be in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. Random initialization is important for symmetry breaking, if all of the initial values are the same the algorithm won't improve on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_init = 0.12\n",
    "\n",
    "# L_in is the number of incoming connections\n",
    "# L_out is the number of outgoing connections\n",
    "def randInitWeights(L_in, L_out): \n",
    "    W = np.random.rand(L_out, L_in+1)\n",
    "    W = W * 2 * epsilon_init\n",
    "    W = W - epsilon_init\n",
    "    return W\n",
    "\n",
    "theta1 = randInitWeights(input_layer_size, hidden_layer_size)\n",
    "theta2 = randInitWeights(hidden_layer_size, num_labels)\n",
    "nn_params = np.concatenate([theta1.flatten(), theta2.flatten()]) # unroll the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, our time has finally come, time to implement the backpropagation algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam):\n",
    "    m = np.size(X, 0)\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1))\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "    D1 = np.zeros(hidden_layer_size, input_layer_size+1)\n",
    "    D2 = np.zeros(num_labels, hidden_layer_size+1)\n",
    "    \n",
    "    # loop through every training example\n",
    "    for t in range(m): \n",
    "        a1 = X[t,:]\n",
    "        z2 = theta1 @ a1\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.insert(a2, 0, 1, axis=0)\n",
    "        z3 = theta2 @ a2\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        yt = np.zeros(num_labels)\n",
    "        for k in range(num_labels):\n",
    "            if(k == y[t][0]):\n",
    "                yt[k] = 1\n",
    "            else:\n",
    "                yt[k] = 0\n",
    "        d3 = a3 - yt\n",
    "\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)\n",
    "        d2 = (theta2.T @ d3)*sigmoidGrad(z2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    t = 0\n",
    "    a1 = X[t,:]\n",
    "    display(a1.shape)\n",
    "    z2 = theta1 @ a1\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=0)\n",
    "    z3 = theta2 @ a2\n",
    "    a3 = sigmoid(z3)\n",
    "    yt = np.zeros(num_labels)\n",
    "    for k in range(num_labels):\n",
    "        if(k == y[t][0]):\n",
    "            yt[k] = 1\n",
    "        else:\n",
    "            yt[k] = 0\n",
    "    d3 = a3 - yt\n",
    "    \n",
    "    z2 = np.insert(z2, 0, 1, axis=0)\n",
    "    d2 = (theta2.T @ d3)*sigmoidGrad(z2)\n",
    "    \n",
    "    D1 = D1 + (d2[1:] @ a1.T)\n",
    "    return D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.05985714e-03,  1.16142137e-02,  8.42203200e-03, -3.12018664e-02,\n",
       "       -7.18254844e-03,  1.33327371e-02,  4.81154560e-02,  3.60730908e-02,\n",
       "        2.28414718e-02,  2.12211484e-02,  3.05915495e-02, -3.60848244e-02,\n",
       "       -2.42928066e-02, -6.08275021e-02, -2.35944417e-02,  3.75753882e-02,\n",
       "        3.17463613e-02,  1.55713532e-02,  1.13364609e-05, -2.48798623e-02,\n",
       "        1.45048585e-02,  2.55911104e-02,  4.54155638e-03, -5.42448479e-03,\n",
       "        1.17954599e-02,  9.51671065e-05])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'D1' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-ef1d1cf74274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-2da13cd378ba>\u001b[0m in \u001b[0;36mbackProp\u001b[0;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mD1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'D1' referenced before assignment"
     ]
    }
   ],
   "source": [
    "result = backProp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, regParam)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
